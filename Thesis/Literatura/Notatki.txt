**lasciate ogni speranza, voi ch’entrate**



Notatki dotyczące przejrzanej literatury etc etc
Główne źródło wiedzy: https://github.com/uzh-rpg/event-based_vision_resources#depth-estimation

gitignore (Ze względów prawnych):
*.pdf
*.mp4



----------------------------------------------------------------------------------------------
Nazwa: Prophesee_FRAMOS_Technical_Summit_2021_Handout & FRAMOS_2021_PROPHESEE
Tytuł: Europe Framos Technical Summit 2021
Autorzy: Philippe Berger
O czym: Konferencja o kamerach zdarzeniowych
Notatki:

-> Mózg reaguje na ruch, aktualizuje informacje

-> Czujniki używają CMOS

=> Nie generują ramek, ale strumień danych - zdarzenia ('events')

=> Każdy piksel pracuje autonomicznie od innych, generuje zdarzenia zależne od zmiany natężenia światła

=> Brak zmian <=> Brak danych

=>Zalety:
	> Brak rozmycia ruchu
	> Bardzo duży zakres dynamiczny ('Dynamic range') >120 dB
	> Bardzo małe opóźnienie ('Latency') - zależy od ilości zmian


--offtopic------	
-> Dynamic range - stosunek pomiędzy najciemniejszym a najjaśniejszym tonem jaki można uzyskać

-> ton obrazu (fototon) - funkcja ilości promieniowania odbitego od odbiektu, które dotarło do sensora i zostało zarejestrowane.

----------------

-> Dużo ciekawych zastosowań

-> O wadach nie mówili... :\

=> Każdy piksel daje informację w formacie XYT
	
----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
Nazwa: https://en.wikipedia.org/ /3D_reconstruction
Tytuł: 3D reconstruction - Wikipedia
Autorzy: God only knows
O czym:	Rekonstrukcja 3D - co to, podstawowe metody
Notatki:
=> Rekonstrukcja 3D: proces uchwycenia kształtu i wyglądu rzeczywistych obiektów. Proces można osiągnąć
metodami aktywnymi lub pasywnymi. Jeśli pozwoli się modelowi zmienić swój kształt w czasie,
 określa się to jako rekonstrukcję niesztywną lub przestrzenno-czasową

-> Cel: kształt 3D, koordynaty każdego jego punktu

-> Metody aktywne: używając map głębi ('depth map') rekonstruują PROFIL 3D poprzez aproksymację numeryczną 
i budują model. W sposób aktywny oddziałowują one na obiekt np. falami radiowymi, świetlnymi
(radar, lidar,sonar etc)

=> Metody pasywne: Nie oddziałowują na obiekt rekonstruowany, używają sensorów do zmierzenia 
promieniowania odbitego albo wyemitowanego przez obiekt(same go nie generują). Metody pasywne można zastosować
do szerszego studium przypadków. Typowo sensor jest to kamera (mamy zdjęcia, eventy).

-> Metody pasywne dzielimy na monokularne('monocular') i stereowizyjne ('stereo vision').

=> Metody monokularne opierają się na jednym bądź więcej zdjęć z jednego ujęcia (kamery). Korzysta z 
charakterystyk 2D (sylwestki, tekstury, cieniowanie) by zmierzyć kształt 3D. Metody to 'Shape-from-shading',
'Photometric stereo' oraz 'shape-from-texture'

-> Shape-from-shadowing: Analiza cienia przy użyciu refleksji Lambertowskiej ('Lambertian reflectance'),
otrzymujemy mapę głębi.

-> Photometric Stereo: Wykonujemy zdjęcia przy różnych warunkach oświetleniowych by uzyskać głębię.

-> Shape-from-texture: Zakładamy, że obiekt ma gładką powierzchnię pokrytą replikowany jednostkami tekstur
(???)('replicated texture unit') a jego rzutowanie z 3D na 2D powoduje zniekształcenie i perspektywę.
Zniekształcenie i perspektywa zmierzone na obrazach 2D dają pewną wskazówkę do odwrotnego rozwiązania głębi i uzyskania informacji o powierzchni obiektu


=> Stereowizja: otrzymujemy geometryczne informacje 3D obiektu na podstawie wielu zdjęć z dwóch kamer
jednocześnie umieszczonych pod różnymi kątami albo z jednej kamery z kilku różnych ujęć pod różnymi kątami.
-> Problem: Większość algorytmów rekonstrukcji 3D jest zbyt powolna i trudna do użycia w czasie rzeczywistym.
-> Opisane metody (do dalszego researchu): Delaunay and alpha-shapes, Zero set Methods, VR Technique, Voxel Grid

----------------------------------------------------------------------------------------------


----------------------------------------------------------------------------------------------
Nazwa: https://en.wikipedia.org/wiki/Computer_stereo_vision
Tytuł: Computer stereo vision
Autorzy: God only knows
O czym: Stereowizja
Notatki:
=> Stereowizja: ekstrakcja informacji 3D ze zdjęć cyfrowych. Porównując dane o scenie z dwóch punktów
widokowych, można uzyskać informacje 3D poprzez analizę pozycji względnych obiektu na dwóch zdjęciach.

-> Porównując dwa zdjęcia uzyskujemy informację o relatywnej głębi ('relative depth') w formie mapy rozbieżności ('disparity map') która koduje różnice w horyzontalnych koordynatach odpowiadających punktach
obrazu. Wartości na tej mapie rozbieżności są odwrotnie proporcjonalne do głębi sceny w odpowiedniej lokalizacji piksela. 

----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
Nazwa: Stereowizja_pol_slonsko
Tytuł: Metody stereowizyjne w analizie składu ziarnowego
Autorzy: Heyduk A
O czym: Opis stereowizji
Notatki:

-> "Świat obserwowany jednym okiem jest płaski, a zdolność do postrzegania wzajemnego położenia obiektów
w przestrzenmi możliwa jest tylko dzięki analizie wzajemnego położenia" - RIP cyclops

-> Pełnia odbioru glębi możliwa jest dopiero przy wykorzystaniu pary oczu (kamer)

-> Punkt centralny rzutowania - punkt w którym przecinają się promienie rzutujące - proste łączące punkty z
ich odpowiednikiem na płsaszczyźnie kamery. Mówimy o tym w kontekście rzutu perspektywicznego 

-> Punkt na płaszczyźnie podstaje jako punkt przecięcia prostej łączącej punkt w przestrzeni i punkt centralny (O1/2)z płaszczyzną obrazu(S1/S2)

-> prostą łączącą punkt centralny K1 z punktem centralnym K2 nazywamy prostą bazową (baseline)

->Punkty przecięcia linii bazowej O1O2 z płaszczyznami obrazów S1/S2 nazywamy punktami epipolarnymi (E1/E2)

-> Dowolny punkt przestrzeni P wraz z punktami E1,E2 tworzy trójkę punktów wyznaczających płaszczyznę nazywaną
płaszczyzną epipolarną.

-> Linie przecięcia płaszczyzny epipolarnej z płaszczyznami obrazów S1 i S2 nazywa się liniami epipolarnymi L1 i L2. 
Linie te łączą obydwa obrazu punktów w obydwu kamerach z odpowiednimi punktami epipolarnymi E1 i E2

-> Uproszczenie znaczne gdy osie optyczne obdwu JEDNAKOWYCH (ta sama ogniskowa i rozdzielczość) kamer
są do siebie równoległe a obrazy S1 i S2 leżą w jednej płaszczyźnie. Jest to układ KANONICZNY



----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
Nazwa: Sterowizja3
Tytuł:SYSTEM OBRAZOWANIA STEREOSKOPOWEGO SEKWENCJI SCEN TRÓJWYMIAROWYCH
Autorzy: DARIUSZ RZESZOTARSKI
O czym: Stereowizja
Notatki:

=>Stereowizja jest technika obrazowa umożliwiajaca wyznaczanie współrzednych punktów sceny trójwymiarowej n 
na podstawie ich obrazów 
uzyskiwanych w co najmniej dwóch urzadzeniach rejestrujacych zwanych dalej kamerami

=> Układ kanoniczny:
a) osie kamer są równoległe
b) współrzędne Z ognisk kamer są takie same


=> Linia epipolarna dla danego punktu pR (pL) z obrazu kamery prawej (lewej) jest linia bedaca obrazem w
kamerze lewej (prawej) prostej wychodzacej z ogniska cR (cL) kamery prawej (lewej)
i przechodzacej przez pR (pL).

=>Dla każdego niekanonicznego układu kamer wszystkie linie epipolarne nie sa wzajemnie
równoległe i przecinaja sie w jednym punkcie zwanym punktem epipolarnym

-> Podstawowe zadanie - wyznaczenie problemu odpowiedniości

=> Zadanie odpowiedniości - wyznaczenie współrzędnych rzutów punktu P na lewą i prawą kamerę

-> W układzie kanonicznym jest to prostsze gdyż wsp Y obydwu rzutów jest jednakowa

-> Rozwiązaniem zadania odpowiedniości jest wyznaczenie OBRAZU DYSPARYCJI (disparity) czyli różnicy
współrzędnych związanych z wzajemnym przesunięciem obrazów każdego punktu przesttrzeni (X,Y,Z) w obu kamerach


=> I tu następuje masa wzorków, całkiem ładnych...

-> W praktycznych zastosowaniach ciężko uzyskać kanoniczny układ kamer. Można jednak stosując odpowiednie 
rzutowanie, przekształcić współrzędne pary obrazów z układu niekanonicznego do układu kanonicznego. Nazywamy
to REKTYFIKACJĄ obrazu.

-> W celu znalezienia pary odpowiednich macierzy przekształcających konieczne jest przeprowadzenie 
kalibracji układu steroewoizjnego ze względu na zewnątrzne oraz wewnętrzne parametry kamer

-> Parametry zewnętrzne kamery opisuje wektor T  określający przesunięcie pomiędzy układem wsp
związanych z kamerą [Xc, Yc, Zc] a arbitralnie wybranym zewnętrznym układem wsp [X,Y,Z] oraz macierz obrotu
R określająca obrót pomiędzy tymi osiami

-> Znajomość parametrów zewnętrznych każdej z kamer jest konieczna do korekcji zniekształceń geometrycznych 
wprowadzanych przez układy optyczne kamer. Można je podzielić na dwie grupy - pierwsza to parametry metryczne
które są dane w tzw. macierzy kamery. Do drugiej z kolei zalicza się parametry określające zniekształcenia 
geometryczne wprowadzone przez układy optyczne kamer. Dzielimy je na radialne i tangencjalne.


-> Potem hokus pokus czary mary i mamy macierze translacji (Ml i Mr) - nie napisali jak xD
----------------------------------------------------------------------------------------------


----------------------------------------------------------------------------------------------
Nazwa: Konspekt rozszerzony 1 Rzutowanie, kalibracja i stereowizja
Tytuł: Rzutowanie, kalibracja, stereowizja
Autorzy: Rotter
O czym: Opis stereowizji
Notatki:

-> współrzędne jednorodne unormowane - [X Y Z 1]^T - poprzez dodanie dodatkowej współrzęnej uzyskujemy
możliwość macierzowego zapisu każdej transformacji, w tym przesunięcia (translacji), co pozwala
na uproszczone składanie przekształceń
-> PRzejścia: zewnętrzny układ wsp -> układ wsp kamery -> układ płaszczyzny obrazowej -> wsp pikselowe ->
ew korekcja

-> Mapa dysparycji - mapa obrazująca dypsarycję czyli różnicę w odległościach punktów na obydwu kamerach 
(im bliżej dany obiekt tym jaśniejszy - większa różnica pomiędzy obydwoma kamerami)

----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
Nazwa: Peter_Hilman_Stereo_Vision
Tytuł: White Paper: Camera Calibration and Stereo Vision
Autorzy: Peter Hilman
O czym: Sterowizja
Notatki:
-> Najważniejsze - sekcja 4 - 3D recovery
-> Bardzo dobra instrukcja - no najlepszy dzisiaj
----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
Nazwa: EventVisionSurvey
Tytuł: Event-based Vision: A Survey
Autorzy: Guillermo Gallego, Tobi Delbr¨uck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi,
Stefan Leutenegger, Andrew J. Davison, J¨org Conradt, Kostas Daniilidis, Davide Scaramuzza
O czym:  Ogólne informacje dotyczące kamer zdarzeniowych oraz ich praktycznego zastosowania
Notatki:
-> Firmy produkujące - Samsung, Prophesee

=> Output kamery: zdarzenie (czas, lokalizacja, znak zmiany)
=> Zalety: brak motion blur, b.duże HDR(High Dynamic Range) - 140db vs 60 db, mała prądożerność, mała latencja

-> Każdy piksel działa niezależnie od pozostałych

-> Działamy na skali logarytmicznej

=> DVS - Dynamic Vision Sensor

-> Designs - DVS/ATIS/DAVIS, ale w sumie na wszystkie mówimy DVS

=> ZMIANA PARADYGMATU xD ,problemy:
1) Output kamer jest asynchroniczny i rzadki
2) Problem z oświetleniem - nie tylko zmiana oświetlenia sceny, ale także obecny i przeszły ruch względny 
kamery i sceny
3) Problem z szumem i efektami dynamicznymi 

=> Zdarzenia powodują przesuwające się krawędzie

=> W zależności od ilości zdarzeń przetwarzanych równocześnie rozróżniamy dwie kategorie stosowanych algorytmów:
1) Metody operujące 'event-by-event basis' gdzie stan systemy (estymowane niewiadome) może się zmienić z 
nadejściem pojedynczego zdarzenia a tym samym osiąga  najmniejszą latencję 
2) Metody operujące na paczkach zdarzeń które wprowadzają pewne opóźnienia

=> Sposoby reprezentacji zdarzeń:
1) Pojedyncze zdarzenia (indyvidual event) - algorytmy event-by-event np. filtry probabilistyczne (???) albo Spiking Neural Networks (ACHTUNG ACHTUNG!!! SNN)
Posiadają one dodatkowe informacje pozyskane z poprzednich zdarzeń albo z dodatkowej wiedzy
2) Pakiety zdarzeń (Event packet) -  pakiety w czaso-przestrzennym sąsiedztwie mogą być przetwarzane razem 
3) Zdarzeniowa ramka albo histogram 2D (Event frame/image or 2D histogram) -
4) Przestrzeń czasowa (Time surface)
5) siatka wokseli (Voxel grid) - Woksele to piksele w 3D
6) Zestaw punktów 3D (3D point set) - mamy X,Y,T więc można przedstawić w przestrzeni geometrycznej
7) Punkty na płaszczyźnie obrazu (Points set on image plane)
8) Obraz zdarzenia z kompensacją ruch (Motion-compensated event image) - w zasadzie to krawędzie 
9) Zrekonstruowany obraz (Reconstructed images)

=> Spiking Neural Networks SNN:
TO DO

=> 3D reconstruction. Monocular and Stereo:
- to szerokie pole (mądre słowa)
A) Instanteous Stereo:
Używamy zdarzeń w bardzo krótkim czasie (sposób per-event basis) z 2 lub więcj sztywno umocowanych
 zsynchronizowanych kamer. Mają one wspólny zegar. Rozwiązanie klasyczne - rozwiązujemy korespondencję stereo
 a potem korzystamy z triangulacji (technika polegająca na rozbicu bardziej złożonych powierzchni na trójkąty)
 
B) Multi-Perspective Panoramas
C) Monocular Depth Estimation
D)Stereo Depth for SLAM
E)Depth Estimation using Structured Light


----------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------
Nazwa:Rebecq2018_Article_EMVSEvent-BasedMulti-ViewStere
Tytuł: EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time
Autorzy: Henri Rebecq · Guillermo Gallego · Elias Mueggler · Davide Scaramuzza
O czym: Rekonstrukcja 3D przy użyciu kamer zdarzeniowych (
Notatki:
 => POJEDYNCZA KAMERA ZDARZENIOWA
 -> Używają EMVS (Event Based Multi View Stereo)
 - > Multi View Stereo - Użycie więcej niż dwóch obrazów

 
----------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------
Nazwa: https://www.youtube.com/watch?v=9mYTbZM9odc
Tytuł: Misha Mahowald’s Beautiful Stereo Chip - Tobi Delbruck- 2020 Telluride Neuromorphic workshop
Autorzy: Misha Mahowald’s
O czym:  stereo fusion work at Caltech in Carver Mead's Physics of Computation lab
Notatki: Nope Nope Nope

----------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------
Nazwa: https://briansimulator.org/
Tytuł: Brian SNN Simulator
Autorzy: ---
O czym: Symulator sieci SNN
Notatki:

=> Sieci SNN są bardzo ciekawe, ale mogą być duże problemy z uczeniem
Wniosek: Może lepiej nie.
----------------------------------------------------------------------------------------------


----------------------------------------------------------------------------------------------
Nazwa: https://www.frontiersin.org/articles/10.3389/fnins.2018.00774/full
Tytuł: Deep Learning With Spiking Neurons: Opportunities and Challenges
Autorzy:  Michael Pfeiffer and Thomas Pfeil
O czym: Spiking Neuron Networks
Notatki:

-> Główna różnica - neurony biologiczne działają z asynchronicznymi pikami które sygnalizują
nadejście charakterystycznego zdarzenia a sztuczne to nieliniowe funkcje ciągłe operujące na 
wspólnym zegarze

-> Sieci neurone nazywamy głębokimi gdy mają co najmniej dwie warstwy ukryte

-> W SNN neurony wymieniają informacje poprzez piki ('spikes'). Z założenia są to zdarzenia, toteż
przetwarzanie informacji sprowadza się do dwóch czynników: czasu (
częstotliwości wypalania, względne taktowanie impulsów przed i postsynaptycznych oraz określone wzorce wystrzeliwania )
oraz tożsamość użytych synaps, tj. które neurony są połączone, czy synapsa jest pobudzająca, 
czy hamująca, siła synaps i możliwa krótkoterminowa plastyczność lub efekty modulacyjne. 

=> Są EVENT -BASED urra!!!
----------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------
Nazwa: 20576
Tytuł: SMARTCAM FOR REAL-TIME STEREO VISION - Address-event Based Embedded System
Autorzy: Stephan Schraml, Peter Schön and Nenad Milosevic
O czym: Estymacja mapy głębi ruszającego się obiektu przy życiu kamery SMART
Notatki:
=> Dane w formacie AER - Address-Event Representation
=> Realizacja na Embedded

-> 2 kamery, kolejka FIFO i procesor DSP

=> Algorytm:
-> 3 kroki : 
	1) kalibracja kamery oraz rektyfikacja
	2) Wyliczenie korespondencji stereo
	3) Rekonstrukcja
	
=> AE - Address-Event (x,y,t,p)
	
-> Wyliczenie parametrów wewnętrznych, zewnętrznych i położenia linii epipolarnych
kamer dokonano przy pomocy Camera Calibration Toolbox korzystającego z nieliniowej funkcji spadku gradientu
-> Bazując na wynikach kalibracji generuje się macierz przemieszczeń (warp matrix) i
adresy Xtev Ytev każdego AE (czyli piksela) są poprawiane w locie by wyeliminować dystorsję i
uzyskać linie epipolarne równoległe do osii poziomej (czyli układ kanoniczny pewnie)

-> Korespondencje stereo wyznaczono używając standardowego (xD) area-based algorytmu modyfikując/readaptując
w celu uzyskania korzyści z AE-based przetwarzania.  Algorytm kojarzący jest aplikowany do każdego zdarzenia
kumulowanego w slocie czasowym o czasie trwania DT
Tak więc aktywność zdarzeniowa (event-activity) wektora pikseli jest przechowywana w formie dynamicznej listy gdzie
częstotliwość zdarzeń jest kodowana w wielkości (magnitude) AEact. Slot czasowy długości DT jest zmienną parametryczną
i może być dobrana, typowo 5ms do 50ms - odpowiedno 200 i 20 fps. Zaczynając od pierwszego aktywnego piksela w lewym obrazku
algorytm szuka najlepszego kandydata w obrazie lewym  używając metryki podobieństwa w przedziale rozbieżności pomiędzy dmin i dmax.
Proces jest następny przetwarzany dla kolejnych AE
----------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------
Nazwa: CORL18_Rebecq
Tytuł: ESIM: an Open Event Camera Simulator
Autorzy: Henri Rebecq, Daniel Gehrig, Davide Scaramuzza
O czym: Symulator ESIM
Notatki:

=> IMU - Inertial Measurement Unit - nawigacja inercyjna

=> Brak stasłego framerate`u -  adaptuje się stosownie do przewidywanych zmian (dynamics) sygnału

-> Rendering engine - oprogramowanie odpowiadające za przetwarzanie zawartości i wyświetlanie tekstu, obrazów na ekranie

=> Irradiance map - Kolekcja punktów w przestrzeni 3D z obliczonym oświetleniem pośrednim w tych punktach

=> Motion Field Map - projekcja 3D relatywnych wektorów ruchu na płaszczyznę 3D. Podobne do Przepływu Optycznego (Optical Flow),
 a w zadzie PO jest aproksymacją MF. (źródło : wikipedia). W symulatorze używane do obliczenia spodziewanej zmiany natężenia światła.



=> Elementy składowe symulatora:
1) Sensor ruchu (Sensor trajectory) - funkcja regularna Tau (smooth function) która mapuje każdą chwilę czasową t do pozycji sensora,
skrętu (prędkości kątowej oraz liniowej) oraz przyspieszenia. Zgodnie z jakąś tam publikacją oznacza się tą pozycję czujnika wyrażoną
w jakimś układzie inercjalnym jako Tw(t) dla ramki W (tutaj dużo zagnieżdżeń), skręt jako Theta(t) a przyspieszenie jako Ba(t).
2) Silnik wyświetlania (rendering engine) - Renderer to funkcja R która mapuje dowolną chwilę czasową t do zrenderowanego obrazu
(albo irradiancji E [strumień promieniowania na jednostkę powierchni]) sceny dla obecnej pozycji sensora. Renderer jest parametryzowany
poprzez środowisko Eta, trajektorię sensora wewnątrz środowiska Tau i konfigurację sensora Omega. Eta kontroluje geometrię sceny jak i jej
zmienność (np. pokój, miasto z samochodami etc). Tau reprezentuje trajektorię kamery wewnątrz środowiska które może być generowane online na podstawie
dynamicznego modelu robota i zestawu funkcji kontrolujących albo prekomputowana offline. Omega reprezentuje konfigurację wirtualnego sensora na którego składają się
parametry wewnętrzne oraz zewnętrzne. Zawiera także parametry specyficzne dla sensora (np. threshold C). Renderer dodatkowo zapewnia pole ruchu V(t).


=> Adaptacyjne techniki próbkowania (Adaptive Sampling Strategies):
-> Bazujące na zmianie jasności (Brightness Change)
Korzystamy z rozwinięcia w szereg Taylora dla kolejnej chwili tk+1 przy wiedzy z Pola Ruchu (Motion Field)
-> Bazujące na rozmieszczeniu pikseli (Pixel Displacement)
Metoda prostsza bazuje na zapewnieniu ograniczenia maksymalnego przemieszczenia piksela pomiędzy dwiema kolejnymi próbkami (renderowanymi klatkami) 
[Tak zdaje się działa symulator v2e].

=> Jest implementacja szumu

=>Testy:
Nie jest idealnie, powody są głównie dwa:
1) Prawdziwy i symulowany setup nie są identyczne :threshold C i poziomy szumu nie są do końca znane 
oraz pozycja i tesktura plakatu nie odpowiadają rzeczywistemu plakatowi używanemu w bazie danych
2) Symulaator używa prostego modelu szumu który nie jest super dokładny

=> Przykładowa aplikacja: Uczenie nadzorowane Przepływu Optycznego

=> Materiał uzupełniający:

A) Trajekotria Sensora (Sensor Trajectory): 
Używamy funkcji ciągłej czasy bazującej na funkcji sklejanej. Użytkownik może wygenerować randomową trajektorię albo załadować
ją z pliku zawierającą dyskretny zestaw póz kamery.

B) Silniki Wyświetlania (Rendering Engines):
- Jest ich kilka
	1) OpenGL Rendering Engine - bazuje na OpenGL tj. na rasteryzacji, implementowany na GPU. Przeznaczony do symulacji dużych ilości
	danych w sposób szybki za cenę symulacji w 3D tylko prostych scen z teksturami.

	2) Photorealistic Rendering Engine - Bazuje na Unreal Engine poprzez projekt UnrealCV
	[open sourcowy projekt do pomocy badaczom sytstemów wizyjnych w budowaniu światów przy użyciu UE]. Fotorealistyczny, ale powolny.

C) Wyliczenie Pola Ruchu (Computation of Motion Field)

D) Dodatkowe Sensory:
	1) Standardowa Kamera
	2) IMU

Kuniec.

----------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------
Nazwa:
Tytuł: 
Autorzy: 
O czym: 
Notatki:
----------------------------------------------------------------------------------------------
